{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66e27e11",
   "metadata": {},
   "source": [
    "# é—®é¢˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c06fc99",
   "metadata": {},
   "source": [
    "ç»™å®šä¸€å°é‚®ä»¶ï¼Œåˆ¤å®šå®ƒæ˜¯å¦å±äºåƒåœ¾é‚®ä»¶ã€‚ç”¨Dè¡¨ç¤ºè¿™å°é‚®ä»¶ï¼Œæ³¨æ„Dç”±Nä¸ªå•è¯ç»„æˆã€‚æˆ‘ä»¬ç”¨y+è¡¨ç¤ºåƒåœ¾é‚®ä»¶ï¼Œy-è¡¨ç¤ºæ­£å¸¸é‚®ä»¶ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94fb0a2",
   "metadata": {},
   "source": [
    "é—®é¢˜çš„æ•°å­¦è¡¨è¾¾ä¸ºï¼š\n",
    "\n",
    "$P(y+|D) = \\frac{P(y+)*P(D|y+)}{P(D)}$\n",
    "\n",
    "$P(y-|D) = \\frac{P(y-)*P(D|y-)}{P(D)}$\n",
    "\n",
    "$P(y+),P(y-)$è¡¨ç¤ºå…ˆéªŒæ¦‚ç‡ï¼Œå³è¡¨ç¤ºé‚®ä»¶åº“é‡Œé¢åƒåœ¾é‚®ä»¶å’Œæ­£å¸¸é‚®ä»¶çš„æ¯”ä¾‹å³å¯ã€‚\n",
    "\n",
    "Dé‡Œé¢æœ‰Nä¸ªå•è¯ $d_1,d_2,...,d_n$\n",
    "\n",
    "$P(D|y+)=P(d_1,d_2,...,d_n|y+)$è¡¨ç¤ºåƒåœ¾é‚®ä»¶ä¸­å‡ºç°å’Œè¿™å°é‚®ä»¶ä¸€æ¨¡ä¸€æ ·çš„æ¦‚ç‡æœ‰å¤šå¤§\n",
    "\n",
    "$P(d_1,d_2,...,d_n|y+)$å¯ä»¥æ‰©å±•ä¸º$P(d_1|y+)P(d_2|d_1,y+)P(d_3|d_1,d_2,y+)...$\n",
    "\n",
    "å‡è®¾$d_i$å’Œ$d_{i-1}$æ˜¯å®Œå…¨æ¡ä»¶æ— å…³çš„(æœ´ç´ è´å¶æ–¯å‡è®¾ç‰¹å¾ä¹‹é—´ç›¸äº’ç‹¬ç«‹ï¼Œäº’ä¸å½±å“)ï¼Œ\n",
    "\n",
    "æ‰©å±•å¯ä»¥ç®€åŒ–ä¸º$P(d_1|y+)P(d_2|y+)P(d_3|y+)...$\n",
    "\n",
    "å¯¹äº$ğ‘ƒ(ğ‘‘1|ğ‘¦+)ğ‘ƒ(ğ‘‘2|ğ‘¦+)ğ‘ƒ(ğ‘‘3|ğ‘¦+)...$ï¼Œåªéœ€è¦ç»Ÿè®¡$d_i$ä¸ªå•è¯åœ¨åƒåœ¾é‚®ä»¶ä¸­å‡ºç°çš„é¢‘ç‡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4eb76a",
   "metadata": {},
   "source": [
    "# ä»£ç å®ç°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b23919a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "\n",
    "def textParse(input_string):\n",
    "    listofTokens = re.split(r\"\\W+\",input_string)\n",
    "    return [token.lower() for token in listofTokens if len(listofTokens) > 2]\n",
    "    \n",
    "def createVocablist(doclist):\n",
    "    vocabSet = Set([])\n",
    "    for document in doclist:\n",
    "        vocabSet = vocabSet | set(document)\n",
    "    return list(vocabSet)\n",
    "    \n",
    "def setOfWord2Vec(vocablist,inputSet):\n",
    "    returnVec = [0]*len(vocablist)\n",
    "    for word in inputSet:\n",
    "        if word in vocablist:\n",
    "            returnVec[vocablist.index(word)] = 1\n",
    "    return returnVec\n",
    "    \n",
    "def trainNB(trainSet,trainClass):\n",
    "    numTrainDocs = len(trainSet)\n",
    "    numWords = len(trainSet[0])\n",
    "    p1 = sum(trainClass)/float(numTrainDocs)  #å…ˆéªŒæ¦‚ç‡ï¼šåƒåœ¾é‚®ä»¶çš„æ¦‚ç‡\n",
    "    p0Num = np.ones(numWords)  #ä¸ç”¨9åˆå§‹åŒ–ï¼Œé¿å…å› ä¸ºæŸä¸ªè¯ä¸å­˜åœ¨ï¼Œå¯¼è‡´æ¦‚ç‡ä¸º0ï¼Œè¿›è€Œå¯¼è‡´æ•´ä¸ªç´¯ä¹˜çš„ç»“æœä¸º0\n",
    "    p1Num = np.ones(numWords)  \n",
    "    p0Denom = 2       #æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ï¼Œä¹Ÿå°±æ˜¯åˆ†æ¯ä¸èƒ½ç”¨0åˆå§‹åŒ–ï¼Œé€šå¸¸è®¾ç½®æˆç±»åˆ«ä¸ªæ•°ï¼Œè¿™æ˜¯æ˜¯2åˆ†ç±»ï¼Œæ‰€ä»¥è®¾ç½®ä¸º2\n",
    "      = 2\n",
    "    for i in rangeï¼ˆnumTrainDocs):\n",
    "        if trainClass[i] == 1: è¡¨ç¤ºåƒåœ¾é‚®ä»¶\n",
    "            p1Num += trainSet[i]   \n",
    "            p1Denom += sum(trainSet[i])   #åˆ†æ¯å¯¹åƒåœ¾é‚®ä»¶ä¸­å‡ºç°çš„å•è¯æ€»æ•°æ±‚å’Œ\n",
    "        else: è¡¨ç¤ºæ­£å¸¸é‚®ä»¶\n",
    "            p0Num += trainSet[i]   \n",
    "            p0Denom += sum(trainSet[i])   #åˆ†æ¯å¯¹æ­£å¸¸é‚®ä»¶ä¸­å‡ºç°çš„å•è¯æ€»æ•°æ±‚å’Œ\n",
    "    \n",
    "    p1Vec = np.log(p1Num/p1Denom)   #è¿™é‡Œçš„æ¦‚ç‡å¯èƒ½å¾ˆå°ï¼Œä½¿ç”¨np.logå°†æ¦‚ç‡å€¼å¯¹%%latexåŒ–\n",
    "    p0Vec = np.log(p0Num/p0Denom)\n",
    "    return p0Vecï¼Œp1Vec,p0Vec,p1\n",
    "   \n",
    "def classifyNB(wordVec,p0Vecï¼Œp1Vec,p1Class):\n",
    "    p1 = log(p1Class)+sum(wordVec*p1Vec)#å¯¹æ•°åŒ–\n",
    "    p0 = log(1-p1Class)+sum(wordVec*p0Vec)#å¯¹æ•°åŒ–\n",
    "    if p0 > p1:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "def spam():\n",
    "    doclist = []\n",
    "    classlist = []\n",
    "    for i in range(1,26):\n",
    "        wordlist = textParse(open(f'email/spam/{i}.txt','r',encoding='utf-8').read())\n",
    "        doclist.append(wordlist)\n",
    "        classlist.appenda(1)  # 1è¡¨ç¤ºåƒåœ¾é‚®ä»¶\n",
    "        \n",
    "        wordlist = textParse(open(f'email/ham/{i}.txt','r',encoding='utf-8').read())\n",
    "        doclist.append(wordlist)\n",
    "        classlist.appenda(0)  # 1è¡¨ç¤ºåƒåœ¾é‚®ä»¶\n",
    "    \n",
    "    vocablist = createVocablist(doclist)\n",
    "    trainSet = list(range(50))\n",
    "    testSet = []\n",
    "    for i in range(10):\n",
    "        randInx = int(random.uniform(0,len(trainSet)))\n",
    "        testSet.append(trainSet[randInx])\n",
    "        del (trainSet[randInx])\n",
    "        \n",
    "    trainMat = []\n",
    "    trainClass = []\n",
    "    for docIndex in trainSet:\n",
    "        trainMat.append(setOfWord2Vec(vocablist,doclist[docIndex]))\n",
    "        trainClass.append(classlist[docIndex])\n",
    "    p0Vecï¼Œp1Vec,p1 = trainNB(np.array(trainMat),np.array(trainClass))\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:\n",
    "        wordVec = setOfWord2Vec(vocablist,doclist[docIndex])\n",
    "        if classifyNB(np.array(wordVec),p0Vecï¼Œp1Vec,p1) != classlist[docIndex]:\n",
    "            errorCount ++\n",
    "    print(f\"å½“å‰10ä¸ªæµ‹è¯•æ ·æœ¬ï¼Œé”™äº†{errorCount}\"ä¸ª)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    spam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce610440",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e464894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "randInx = int(random.uniform(0,50))\n",
    "randInx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c4aa4a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The competition is so intense that some say theyâ€™ve given up on their dreams and aspirations.\\nIn March this year, another Chinese term emerged online. Reflecting an attitude toward life, the term â€œbai lanâ€ is translated to mean â€œlet it rot.â€ Posts related to the topic have garnered more than 91 million views on Chinese social media giant Weibo as of Wednesday. \\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=1\n",
    "open(f'email/spam/{i}.txt','r',encoding='utf-8').read()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
