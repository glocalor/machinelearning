{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01ee10b1",
   "metadata": {},
   "source": [
    "# 梯度的定义"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d09316",
   "metadata": {},
   "source": [
    "由全部变量的偏导数汇总\n",
    "而成的向量称为梯度（gradient）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de34ca4c",
   "metadata": {},
   "source": [
    "# 提前准备的函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a34b0f",
   "metadata": {},
   "source": [
    "## softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88d302cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def softmax(a):\n",
    "#    c = np.max(a)\n",
    "#    exp_a = np.exp(a - c) # 溢出对策\n",
    "#    sum_exp_a = np.sum(exp_a)\n",
    "#    y = exp_a / sum_exp_a\n",
    "#    return y\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        print('传入softmax的是2维向量')\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T  #返回的是一维的向量\n",
    "\n",
    "    x = x - np.max(x) # 溢出对策\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ac072f",
   "metadata": {},
   "source": [
    "## mini-batch版交叉熵误差的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2081163e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):#返回的结果值是个标量\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y + 1e-7)) / batch_size   #除以batch_size，表示结果是求取平均每个样本的交叉熵"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100ce2df",
   "metadata": {},
   "source": [
    "# 数值微分法_基于数值微分计算参数的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7661f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f,x):\n",
    "    h = 1e-4\n",
    "    return((f(x+h)-f(x-h))/(2*h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ae53ebb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f,x):\n",
    "    h = 1e-4 #0.0001\n",
    "    grad = np.zeros_like(x) # 生成和x形状相同的数组\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        # f(x+h)的计算\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)  #目标变量改变+h，其他变量不变，传入函数中，得到标量值\n",
    "        \n",
    "        # f(x-h)的计算\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x) #目标变量改变-h，其他变量不变，传入函数中，得到标量值\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val # 还原值\n",
    "        \n",
    "    return grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bdbbdcfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 8.])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#测试\n",
    "def function_2(x):\n",
    "    return x[0]**2+x[1]**2\n",
    "\n",
    "numerical_gradient(function_2, np.array([3.0, 4.0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dc25fe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b744cd",
   "metadata": {},
   "source": [
    "参数f是要进行最优化的函数，init_x是初始值，lr是学习率learning rate，step_num是梯度法的重复次数。\n",
    "\n",
    "numerical_gradient(f,x)会求函数的梯度，用该梯度乘以学习率得到的值进行更新操作，由step_num指定重复的次数。\n",
    "\n",
    "使用这个函数可以求函数的极小值，顺利的话，还可以求函数的最小值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69168c7",
   "metadata": {},
   "source": [
    "问题：请用梯度法求${f(x_0,x_1)=x_0^2+x_1^2}$ 的最小值。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f985686e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.11110793e-10,  8.14814391e-10])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2+x[1]**2\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2,init_x,lr=0.1, step_num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c755c50",
   "metadata": {},
   "source": [
    "## 神经网络的梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ffc096",
   "metadata": {},
   "source": [
    "### 一个简单的神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b5f95a5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.23210343,  0.80529943,  0.33053336],\n",
       "       [ 1.15018482,  0.39713218, -1.69789563]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3) # 用高斯分布进行初始化\n",
    "        \n",
    "    def predict(self,x):\n",
    "        return np.dot(x,self.W)\n",
    "   \n",
    "    def loss(self,x,t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "net = simpleNet()\n",
    "net.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8ad37c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.1744284   0.84059862 -1.32978605]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a047344d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "58932304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.090840170292014"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([0, 0, 1]) # 正确解标签\n",
    "net.loss(x,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "194c2517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.33372796  0.23899351 -0.57272147]\n",
      " [ 0.50059194  0.35849026 -0.8590822 ]]\n"
     ]
    }
   ],
   "source": [
    "def f(W):\n",
    "    return net.loss(x, t)\n",
    "\n",
    "dW = numerical_gradient(f, net.W)  #这里带入的参数必须是net.W。这样，对net.W中的值的更改才能实现对f函数中的loss函数的值的更改。\n",
    "#这儿是一个核心点\n",
    "# 这个函数的理解，表面看f(W)的函数内并没有使用W，实际上是隐藏在了loss函数中，\n",
    "# numerical_gradient函数使得参数net.W矩阵依次更改每个位置的值，即+h，或-h，会使得loss函数计算使用新的W，得出新的值，\n",
    "#继而实现数值微分方式下net.W每个位置处的偏导数\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057ac4f4",
   "metadata": {},
   "source": [
    "### 2层神经网络的类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b06aeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self,input_size,hidden_size,output_size,weight_init_std=0.01):\n",
    "        #初始化权重\n",
    "        self.params={}\n",
    "        self.params['W1'] = weight_init_std*np.random.randn(input_size,hidden_size)\n",
    "        self.params['b1'] = weight_init_std*np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std*np.random.randn(hidden_size,output_size)\n",
    "        self.params['b2'] = weight_init_std*np.zeros(output_size)\n",
    "    \n",
    "    def predict(self,x,t):\n",
    "        W1,W2 = self.params['W1'],self.params['W2']\n",
    "        b1,b2 = self.params['b1'],self.params['b2']\n",
    "        \n",
    "        a1 = np.dot(x,W1)+b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y \n",
    "        \n",
    "    \n",
    "    def loss(self,x,t):\n",
    "        y = self.predict(x,t)\n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self,x,t):\n",
    "        y = self.predict(x,t)\n",
    "        yout = np.argmax(y,axis=1)\n",
    "        tout = np.argmax(t,axis=1)\n",
    "        return np.sum(yout==tout)/float(x.shape[0])\n",
    "    \n",
    "    def numerical_gradient(self,x,t):\n",
    "        #先定义要求梯度的函数，这里就是损失函数，但注意不是直接调用损失函数计算值\n",
    "        loss_W = lambda W:self.loss(x,t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W,self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W,self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W,self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W,self.params['b2'])\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c95956a",
   "metadata": {},
   "source": [
    "### mini-batch的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520083ac",
   "metadata": {},
   "source": [
    "以TwoLayerNet类为对象，使用MNIST数据集进行学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da91818",
   "metadata": {},
   "outputs": [],
   "source": [
    "#很费时间-不建议运行\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label = True)\n",
    "\n",
    "train_loss_list = []\n",
    "\n",
    "# 超参数\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 获取mini-batch\n",
    "    batch_mask = np.random.choice(train_size, batch_size) #从0到train_size-1中随机选择batch_size个序号\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 计算梯度\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    # grad = network.gradient(x_batch, t_batch) # 高速版!\n",
    "    \n",
    "    # 更新参数\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 记录学习过程\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6baaad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#每经过一个epoch会输出当前的训练样本准确率和测试样本准确率\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label = True)\n",
    "\n",
    "# 超参数\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "iter_per_epoch = max(x_train.shape[0]/batch_size,1)  #每用完所有样本来测试所需要的次数\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    print(f'序号:{i}')\n",
    "    # 获取mini-batch\n",
    "    batch_mask = np.random.choice(train_size, batch_size) #从0到train_size-1中随机选择batch_size个序号\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 计算梯度\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    # grad = network.gradient(x_batch, t_batch) # 高速版!\n",
    "    \n",
    "    # 更新参数\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 记录学习过程\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    # 计算每个epoch的识别精度\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc13f6e",
   "metadata": {},
   "source": [
    "# 误差反向传播法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c162e2",
   "metadata": {},
   "source": [
    "反向传播的核心是利用链式法则，对每一个参数(${\\omega}$)的求导，转化为对组合成复合函数的多个简单函数（乘法、加法）的求导的乘积"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb113b3",
   "metadata": {},
   "source": [
    "## 乘法层的正向和反向传播实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4e0adcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulLayer():\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        \n",
    "    def forward(self,x,y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x*y\n",
    "        return out\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        dx = dout*self.y\n",
    "        dy = dout*self.x\n",
    "        \n",
    "        return dx,dy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4703e7",
   "metadata": {},
   "source": [
    "### 以购买苹果为例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4906ed99",
   "metadata": {},
   "source": [
    "苹果的单价*为100，\n",
    "\n",
    "购买个数为2，\n",
    "\n",
    "消费税为10%(转换成如图的乘法后，则为1.1)\n",
    "\n",
    "那么\n",
    "\n",
    "价格=苹果的单价*个数*消费税=220"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d8d636",
   "metadata": {},
   "source": [
    "![图片](images/apple_buying.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4a252f",
   "metadata": {},
   "source": [
    "该图由2个乘法层实现，这是的苹果单价、个数、消费税就是我们需要求导数的变量\n",
    "\n",
    "用数学表达式来说明\n",
    "\n",
    "即${y=x_1*x_2*x_3}$\n",
    "\n",
    "其中:\n",
    "\n",
    "${x_1}$表示苹果的价格\n",
    "\n",
    "${x_2}$表示苹果的数量\n",
    "\n",
    "${x_3}$表示消费税\n",
    "\n",
    "现在需要做的就是求偏导数：${dx_1,dx_2,dx_3}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e145b5",
   "metadata": {},
   "source": [
    "### 正向传播时记录输入数据，以便在反向传播时应用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac2deb17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220.00000000000003\n",
      "2.2 110.00000000000001 200\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "# layer\n",
    "mul_apple_layer = Mullayer()\n",
    "mul_tax_layer = Mullayer()\n",
    "\n",
    "#forward\n",
    "apple_price = mul_apple_layer.forward(apple,apple_num)\n",
    "price = mul_tax_layer.forward(apple_price,tax)\n",
    "\n",
    "print(price) \n",
    "\n",
    "#backward\n",
    "dprice = 1\n",
    "dapple_price,dtax = mul_tax_layer.backward(dprice)\n",
    "dapple,dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "print(dapple,dapple_num,dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9655c084",
   "metadata": {},
   "source": [
    "## 加法层的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d849299",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayer():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def forward(self,x,y): \n",
    "        out = x + y\n",
    "        return out\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        dx = dout*1\n",
    "        dy = dout*1\n",
    "        return dx,dy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b91458",
   "metadata": {},
   "source": [
    "### 以购买苹果和橘子为例子"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50890e8",
   "metadata": {},
   "source": [
    "![图片](images/apple_orange_buying.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e43c4964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715.0000000000001\n",
      "2.2 110.00000000000001 3.3000000000000003 165.0 650\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "#layers\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "#forward\n",
    "apple_price = mul_apple_layer.forward(apple,apple_num)\n",
    "orange_price = mul_orange_layer.forward(orange,orange_num)\n",
    "apple_orange_price = add_apple_orange_layer.forward(apple_price,orange_price)\n",
    "price = mul_tax_layer.forward(apple_orange_price,tax)\n",
    "print(price)\n",
    "\n",
    "#backward\n",
    "dprice = 1\n",
    "dapple_orange_price,dtax = mul_tax_layer.backward(dprice)\n",
    "dapple_price,dorange_price = add_apple_orange_layer.backward(dapple_orange_price)\n",
    "dapple,dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "dorange,dorange_num = mul_orange_layer.backward(dorange_price)\n",
    "print(dapple,dapple_num,dorange,dorange_num,dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab24cfa",
   "metadata": {},
   "source": [
    "## 执行步骤总结"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540f57d4",
   "metadata": {},
   "source": [
    "- 首先，生成必要的层，以合适的顺序调用正向传播的forward()方法。\n",
    "- 然后，用与正向传播相反的顺序调用反向传播的backward()方法，就可以求出想要的导数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5423be93",
   "metadata": {},
   "source": [
    "## 激活函数层的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadc7f38",
   "metadata": {},
   "source": [
    "### ReLU层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32da0691",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu():\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self,x):  #注意x是个NumPy对象\n",
    "        self.mask = (x<=0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b870b308",
   "metadata": {},
   "source": [
    "### Sigmoid层实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ced3167d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid():\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = sigmoid(x)\n",
    "        self.out = out\n",
    "        return out\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5627825",
   "metadata": {},
   "source": [
    "这个实现中，正向传播时将输出保存在了实例变量out中。然后，反向\n",
    "传播时，使用该变量out进行计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15aeff3",
   "metadata": {},
   "source": [
    "## Affine层的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c77032",
   "metadata": {},
   "source": [
    "神经网络的正向传播中，为了计算加权信号的总和，使用了矩阵的乘积运算（NumPy中是np.dot())\n",
    "\n",
    "神经元的加权和可以用${Y = np.dot(X, W) + B}$计算出来。然后，Y 经过激活函数转换后，传递给下一层。\n",
    "\n",
    "注：神经网络的正向传播中进行的矩阵的乘积运算在几何学领域被称为“仿射变换”。因此，这里将进行仿射变换的处理实现为“Affine层”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6250969",
   "metadata": {},
   "source": [
    "### 批版本的Affine层\n",
    "\n",
    "考虑N个数据一起进行正向传播的情况，也就是批版本的Affine层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b349949",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine_old():\n",
    "    def __init__(self,W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self,x):\n",
    "        self.x = x\n",
    "        out = np.dot(x,self.W)+self.b\n",
    "        return out\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        dx = np.dot(dout,self.W.T)\n",
    "        dW = np.dot(self.x.T,dout)\n",
    "        db = np.sum(dout,axis=0)\n",
    "        return dx       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95445ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#相比old版本，新增了对张量的处理代码\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W =W\n",
    "        self.b = b\n",
    "        \n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "        # 权重和偏置参数的导数\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 对应张量\n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "\n",
    "        out = np.dot(self.x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        dx = dx.reshape(*self.original_x_shape)  # 还原输入数据的形状（对应张量）\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac27a6b",
   "metadata": {},
   "source": [
    "## Softmax-with-Loss 层 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6ccc6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None # 损失\n",
    "        self.y = None # softmax的输出\n",
    "        self.t = None #监督数据\n",
    "        \n",
    "    def forward(self,x,t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self,dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        \n",
    "        if self.t.size == self.y.size: # 监督数据是one-hot-vector的情况\n",
    "            dx = (self.y - self.t) / batch_size\n",
    "        else:\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arange(batch_size), self.t] -= 1\n",
    "            dx = dx / batch_size\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5cde78",
   "metadata": {},
   "source": [
    "## 实现两层神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a844398",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self,input_size,hidden_size,output_size,weight_init_std=0.01):\n",
    "        \n",
    "        #初始化权重\n",
    "        self.params={}\n",
    "        self.params['W1'] = weight_init_std * np.random.rand(input_size,hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.rand(hidden_size,output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "        # 生成层\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'],self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'],self.params['b2'])\n",
    "        \n",
    "        self.lastlayer = SoftmaxWithLoss()\n",
    "    \n",
    "    def predict(self,x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    # x:输入数据, t:监督数据\n",
    "    def loss(self,x,t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastlayer.forward(y,t)\n",
    "    \n",
    "    # x:输入数据, t:监督数据\n",
    "    def accuracy(self,x,t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)  #获取输出结果y中每一行(一行表示一个样本的结果值)中最大值的索引\n",
    "        if t.ndim != 1 : \n",
    "            t = np.argmax(t, axis=1)  #获取监督数据t中每一行中最大的索引\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])   #分母表示验证数据的大小\n",
    "        return accuracy\n",
    "    \n",
    "     # x:输入数据, t:监督数据\n",
    "    def numerical_gradient(self,x,t):\n",
    "        pass\n",
    "    \n",
    "     # x:输入数据, t:监督数据\n",
    "    def gradient(self,x,t):\n",
    "        self.loss(x,t)  #触发正向传播\n",
    "        dout = 1\n",
    "        dout = self.lastlayer.backward(dout)\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "            \n",
    "        # 设定\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "        \n",
    "        return grads\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94339c12",
   "metadata": {},
   "source": [
    "## mini-batch的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c29e6da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc | 0.10218333333333333, 0.101\n",
      "train acc, test acc | 0.8995, 0.9024\n",
      "train acc, test acc | 0.9169166666666667, 0.9193\n",
      "train acc, test acc | 0.9314833333333333, 0.9317\n",
      "train acc, test acc | 0.9431, 0.9423\n",
      "train acc, test acc | 0.94935, 0.9483\n",
      "train acc, test acc | 0.9548, 0.9526\n",
      "train acc, test acc | 0.9568833333333333, 0.9552\n",
      "train acc, test acc | 0.9597333333333333, 0.9562\n",
      "train acc, test acc | 0.9644, 0.96\n",
      "train acc, test acc | 0.9675666666666667, 0.9616\n",
      "train acc, test acc | 0.9701166666666666, 0.9633\n",
      "train acc, test acc | 0.9707, 0.9648\n",
      "train acc, test acc | 0.9724, 0.9654\n",
      "train acc, test acc | 0.9749166666666667, 0.967\n",
      "train acc, test acc | 0.9757333333333333, 0.966\n",
      "train acc, test acc | 0.97635, 0.9677\n"
     ]
    }
   ],
   "source": [
    "#import sys, os\n",
    "#sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.layers import Affine\n",
    "#from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label = True)\n",
    "\n",
    "# 超参数\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "iter_per_epoch = max(x_train.shape[0]/batch_size,1)  #每用完所有样本来测试所需要的次数\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 获取mini-batch\n",
    "    batch_mask = np.random.choice(train_size, batch_size) #从0到train_size-1中随机选择batch_size个序号\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 计算梯度\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    # grad = network.gradient(x_batch, t_batch) # 高速版!\n",
    "    # 更新参数\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 记录学习过程\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    # 计算每个epoch的识别精度\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfe8a5a",
   "metadata": {},
   "source": [
    "# 卷积神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b80165",
   "metadata": {},
   "source": [
    "卷积神经网络（Convolutional Neural Network，CNN）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea4193c",
   "metadata": {},
   "source": [
    "## 卷积层的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2107c89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        # 中间数据（backward时使用）\n",
    "        self.x = None   \n",
    "        self.col = None\n",
    "        self.col_W = None\n",
    "        \n",
    "        # 权重和偏置参数的梯度\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape # FN：滤波器个数，C：通道数，FH：滤波器高督，FW滤波器宽度\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
    "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
    "\n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)  #使用im2col将4维度（N, C, H, W）的图像数据转化为二维的矩阵，大小为（N*out_w*out_h，滤波器大小）\n",
    "        col_W = self.W.reshape(FN, -1).T  #将C个滤波器中每一个变成一行，形成(FN，滤波器大小)的矩阵，转置后形成(滤波器大小，FN)的矩阵\n",
    "\n",
    "        out = np.dot(col, col_W) + self.b  # out为(N*out_w*out_h,FN)，b的形状为（1，FN）\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)  #输出为N个形状为(out_h, out_w,FN)的矩阵，\n",
    "                                                                    #transpose调整为形状为(N，FN，out_h,out_w)的矩阵\n",
    "            \n",
    "        self.x = x\n",
    "        self.col = col\n",
    "        self.col_W = col_W\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)  #将后向传播的输入的形状变为(N*out_h*out_w,FN)\n",
    "\n",
    "        self.db = np.sum(dout, axis=0)   #db就是dout对每一列求和\n",
    "        self.dW = np.dot(self.col.T, dout)  # dW就是矩阵乘积\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)  #变换成原始的形状\n",
    "\n",
    "        dcol = np.dot(dout, self.col_W.T)   #求出dcol的值即为dout和self.col_W.T的矩阵乘积\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)  #dcol由二维转换回去形状（N, C, H, W ），即为dx？？\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a92a8c",
   "metadata": {},
   "source": [
    "## pooling层的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c875f19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        self.x = None\n",
    "        self.arg_max = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)  #将形状调整为(N*out_h*out_w*C,pool_h*pool_w),pool_h*pool_w表示池化的大小，也就是一行表示一个要取池化结果的数据行\n",
    "\n",
    "        arg_max = np.argmax(col, axis=1)   #取出每一行中的最大值所在索引，为提取池化操作结果做准备\n",
    "        out = np.max(col, axis=1)       # 形状为(N*out_h*out_w*C,1)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)   #输出结果为(N,C,out_h,out_w)\n",
    "\n",
    "        self.x = x\n",
    "        self.arg_max = arg_max\n",
    "\n",
    "        return out\n",
    "\n",
    "    # dout表示4维的张量\n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0, 2, 3, 1)  #将反向传播的输入转化为(N,out_h,out_w,C)\n",
    "        \n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        dmax = np.zeros((dout.size, pool_size))  #dmax的形状为(N*out_h*out_w*C,pool_h*pool_w),并且使用0初始化dx\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()  #dmax将正向传播时col取最大值的位置设置为dout相同位置的值\n",
    "                                                                                    #该操作可以理解为取最大值的位置偏导数为1，让dout正常通过，其他位置为0，dout无法通过\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,))  #dmax形状变为(N,out_h,out_w,C,pool_size)\n",
    "        \n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)  #形状变为二维(N*out_h*out_w,C*pool_size)\n",
    "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad) #dcol由二维转换回去形状（N, C, H, W ），即为dx？？\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394f1ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization:\n",
    "    \"\"\"\n",
    "    http://arxiv.org/abs/1502.03167\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma, beta, momentum=0.9, running_mean=None, running_var=None):\n",
    "        self.gamma = gamma\n",
    "        self.beta = beta\n",
    "        self.momentum = momentum\n",
    "        self.input_shape = None # Conv层的情况下为4维，全连接层的情况下为2维  \n",
    "\n",
    "        # 测试时使用的平均值和方差\n",
    "        self.running_mean = running_mean\n",
    "        self.running_var = running_var  \n",
    "        \n",
    "        # backward时使用的中间数据\n",
    "        self.batch_size = None\n",
    "        self.xc = None\n",
    "        self.std = None\n",
    "        self.dgamma = None\n",
    "        self.dbeta = None\n",
    "\n",
    "    def forward(self, x, train_flg=True):\n",
    "        self.input_shape = x.shape\n",
    "        if x.ndim != 2:\n",
    "            N, C, H, W = x.shape\n",
    "            x = x.reshape(N, -1)   #如果数据是4维(也就是接在卷积层之后)，将其变为2维\n",
    "\n",
    "        out = self.__forward(x, train_flg)\n",
    "        \n",
    "        return out.reshape(*self.input_shape)  #将输出形状变为和输入相同形状\n",
    "            \n",
    "    def __forward(self, x, train_flg):\n",
    "        if self.running_mean is None:\n",
    "            N, D = x.shape\n",
    "            self.running_mean = np.zeros(D)\n",
    "            self.running_var = np.zeros(D)\n",
    "                        \n",
    "        if train_flg:\n",
    "            mu = x.mean(axis=0)   #对每一列求均值\n",
    "            xc = x - mu           \n",
    "            var = np.mean(xc**2, axis=0)  \n",
    "            std = np.sqrt(var + 10e-7)\n",
    "            xn = xc / std        #将x的每一个值变为标准正太分布\n",
    "            \n",
    "            self.batch_size = x.shape[0]\n",
    "            self.xc = xc\n",
    "            self.xn = xn\n",
    "            self.std = std\n",
    "            self.running_mean = self.momentum * self.running_mean + (1-self.momentum) * mu\n",
    "            self.running_var = self.momentum * self.running_var + (1-self.momentum) * var            \n",
    "        else:\n",
    "            xc = x - self.running_mean\n",
    "            xn = xc / ((np.sqrt(self.running_var + 10e-7)))\n",
    "            \n",
    "        out = self.gamma * xn + self.beta   #输出为二维矩阵\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        if dout.ndim != 2:\n",
    "            N, C, H, W = dout.shape\n",
    "            dout = dout.reshape(N, -1)   \n",
    "\n",
    "        dx = self.__backward(dout)\n",
    "\n",
    "        dx = dx.reshape(*self.input_shape)\n",
    "        return dx\n",
    "\n",
    "    def __backward(self, dout):\n",
    "        dbeta = dout.sum(axis=0)\n",
    "        dgamma = np.sum(self.xn * dout, axis=0)\n",
    "        dxn = self.gamma * dout\n",
    "        dxc = dxn / self.std\n",
    "        dstd = -np.sum((dxn * self.xc) / (self.std * self.std), axis=0)\n",
    "        dvar = 0.5 * dstd / self.std\n",
    "        dxc += (2.0 / self.batch_size) * self.xc * dvar\n",
    "        dmu = np.sum(dxc, axis=0)\n",
    "        dx = dxc - dmu / self.batch_size\n",
    "        \n",
    "        self.dgamma = dgamma\n",
    "        self.dbeta = dbeta\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0059ce02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3, 4, 4)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1,2,3,4)+(4,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "284.294px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
